{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbFBkPkkRGg3XUNR/jSN+q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Understanding the Limitations of RNNs and LSTMs\n","Before diving into self-attention models, it's important for the students to understand why the field moved beyond RNNs and LSTMs.\n","\n","## Issues with RNNs/LSTMs:\n","- **Sequential Processing:** RNNs and LSTMs process data sequentially, which makes parallelization (and hence, faster training and inference) difficult.\n","- **Long-Term Dependencies:** Even with LSTMs, learning long-term dependencies in very long sequences can be challenging due to the vanishing gradient problem.\n","- **Complexity and Training Time:** These models can become quite complex and may take a long time to train on large datasets."],"metadata":{"id":"wyk2s-w7MG0y"}},{"cell_type":"markdown","source":["### Example: LSTM Struggling with Long-Context Word Prediction\n","#### Scenario:\n","We'll use sentences where a specific word early in the sentence determines the choice of a word later in the sentence. For example, in the sentence \"I grew up in France... I speak fluent **French**\", the word \"French\" is highly dependent on the word \"France\" mentioned earlier. Let's see if we can predict it."],"metadata":{"id":"vO2tBfuzNzDZ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"iSDPPdM-LqV0","executionInfo":{"status":"ok","timestamp":1700180091521,"user_tz":480,"elapsed":15849,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7f79a66-d44e-4cf8-9064-9315ba69390c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-0bc0898bc6f9>:69: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  return torch.tensor(sequences, dtype=torch.long), torch.tensor(sequence_lengths, dtype=torch.long)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.5977\n","Epoch 2, Loss: 1.8552\n","Epoch 3, Loss: 1.0975\n","Epoch 4, Loss: 0.5490\n","Epoch 5, Loss: 0.2857\n","Epoch 6, Loss: 0.1703\n","Epoch 7, Loss: 0.1139\n","Epoch 8, Loss: 0.0823\n","Epoch 9, Loss: 0.0621\n","Epoch 10, Loss: 0.0482\n","Epoch 11, Loss: 0.0385\n","Epoch 12, Loss: 0.0316\n","Epoch 13, Loss: 0.0265\n","Epoch 14, Loss: 0.0226\n","Epoch 15, Loss: 0.0196\n","Epoch 16, Loss: 0.0172\n","Epoch 17, Loss: 0.0153\n","Epoch 18, Loss: 0.0137\n","Epoch 19, Loss: 0.0123\n","Epoch 20, Loss: 0.0112\n","Epoch 21, Loss: 0.0102\n","Epoch 22, Loss: 0.0094\n","Epoch 23, Loss: 0.0086\n","Epoch 24, Loss: 0.0080\n","Epoch 25, Loss: 0.0074\n","Epoch 26, Loss: 0.0069\n","Epoch 27, Loss: 0.0065\n","Epoch 28, Loss: 0.0061\n","Epoch 29, Loss: 0.0057\n","Epoch 30, Loss: 0.0054\n","Epoch 31, Loss: 0.0051\n","Epoch 32, Loss: 0.0048\n","Epoch 33, Loss: 0.0045\n","Epoch 34, Loss: 0.0043\n","Epoch 35, Loss: 0.0041\n","Epoch 36, Loss: 0.0039\n","Epoch 37, Loss: 0.0037\n","Epoch 38, Loss: 0.0035\n","Epoch 39, Loss: 0.0034\n","Epoch 40, Loss: 0.0032\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import Counter\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","# Sample sentences\n","sentences = [\n","    \"I grew up in France, I speak fluent French\",\n","    \"She was born in Spain and speaks Spanish\",\n","    # ... more sentences ...\n","]\n","\n","# Preprocessing\n","def tokenize(sentences):\n","    tokens = [sent.lower().split() for sent in sentences]\n","    return tokens\n","\n","tokens = tokenize(sentences)\n","all_words = [word for sent in tokens for word in sent]\n","vocab = list(set(all_words))\n","word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x, lengths):\n","        x = self.embedding(x)\n","        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n","        packed_output, _ = self.lstm(packed_input)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","        out = self.fc(output)\n","        return out\n","\n","    def predict_next_word(self, sentence):\n","        self.eval()  # Set the model to evaluation mode\n","        tokens = sentence.lower().split()\n","        idxs = [word_to_idx.get(word, 0) for word in tokens]  # Convert words to indices\n","        seq = torch.tensor(idxs, dtype=torch.long).unsqueeze(0)  # Convert to tensor\n","        with torch.no_grad():\n","            output = self(seq, [len(idxs)])\n","            last_word_logits = output[0, -1]\n","            predicted_idx = torch.argmax(last_word_logits).item()\n","        return vocab[predicted_idx]  # Return the predicted word\n","\n","# Constants\n","VOCAB_SIZE = len(vocab)\n","EMBED_SIZE = 100\n","HIDDEN_SIZE = 128\n","\n","# Instantiate the model\n","model = LSTMModel(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n","\n","# Prepare data for training\n","def create_dataset(tokens):\n","    sequences = []\n","    sequence_lengths = []\n","    for sentence in tokens:\n","        idxs = [word_to_idx[word] for word in sentence]\n","        for i in range(1, len(idxs)):\n","            sequences.append(idxs[:i+1])\n","            sequence_lengths.append(i+1)\n","    sequences = [np.pad(seq, (0, max(sequence_lengths)-len(seq)), mode='constant') for seq in sequences]\n","    return torch.tensor(sequences, dtype=torch.long), torch.tensor(sequence_lengths, dtype=torch.long)\n","\n","sequences, seq_lengths = create_dataset(tokens)\n","\n","# Training setup\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","def train(model, data, epochs):\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for seq, length in zip(*data):\n","            optimizer.zero_grad()\n","            output = model(seq.unsqueeze(0), [length])\n","            loss = criterion(output.squeeze(0)[:length-1], seq[1:length])\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        avg_loss = total_loss / len(data[0])\n","        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')\n","\n","\n","# Run training\n","train(model, (sequences, seq_lengths), 40)"]},{"cell_type":"code","source":["def test_model_on_sentences(model, test_sentences):\n","    for sentence in test_sentences:\n","        prediction = model.predict_next_word(sentence)\n","        print(f\"Sentence: '{sentence}' -> Predicted next word: '{prediction}'\")\n","\n","# Example sentences with important context at different positions\n","test_sentences = [\n","    \"I grew up in France, I speak fluent\",\n","    \"She was born in Spain and speaks\",\n","    \"In Germany, many people speak\",\n","    # Longer context sentences\n","    \"After spending a decade in Japan, I finally learned to speak\",\n","    \"Listen to me, because I will only say this once: when you travel the world you will learn many different things. Once thing you will\"\n","]\n","\n","test_model_on_sentences(model, test_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wu5z872zPsrW","executionInfo":{"status":"ok","timestamp":1700180119022,"user_tz":480,"elapsed":161,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"36b682e8-1a36-40bf-b8f8-b4efd99eb2cd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: 'I grew up in France, I speak fluent' -> Predicted next word: 'french'\n","Sentence: 'She was born in Spain and speaks' -> Predicted next word: 'spanish'\n","Sentence: 'In Germany, many people speak' -> Predicted next word: 'in'\n","Sentence: 'After spending a decade in Japan, I finally learned to speak' -> Predicted next word: 'fluent'\n","Sentence: 'Listen to me, because I will only say this once: when you travel the world you will learn many different things. Once thing you will' -> Predicted next word: 'up'\n"]}]},{"cell_type":"markdown","source":["## Explanation\n","\n","- **Model:** This LSTM model uses word embeddings and is designed to predict the next word in a sequence.\n","- **Data Preparation:** We preprocess the sentences by tokenizing them and creating a vocabulary. The model is trained on sequences of increasing length, predicting the next word at each step.\n","- **Training:** During training, the model receives partial sentences and tries to predict the next word. The training loop includes packing the sequences to handle variable lengths.\n","- **Prediction Function:** The predict_next_word method takes a partial sentence, processes it through the model, and outputs the model's prediction for the next word.\n","- **Testing with Varied Contexts:** The test_model_on_sentences function tests the model with different sentences where the key context for predicting the next word (the name of a country) is located at varying distances from the end of the sentence."],"metadata":{"id":"7t-jmme2QVhX"}},{"cell_type":"markdown","source":["# But what are embeddings?\n","\n","Some resources:\n","- Vicki Boykis [What Are Embeddings](https://vickiboykis.com/what_are_embeddings/)\n","- Simon Wilson [What Are Embeddings? Why They Matter](https://simonwillison.net/2023/Oct/23/embeddings/)\n","- Roy Keyes [The Shortest Definition of Embeddings?](https://roycoding.com/blog/2022/embeddings.html)\n","\n","\n","Today, we're going to at this wonderful article: Visual Storytelling Team and Madhumita Murgia in (randomly?) Fortune Magazine [Generative AI](https://ig.ft.com/generative-ai/)"],"metadata":{"id":"1EBwtjHDQ00r"}},{"cell_type":"markdown","source":["Embeddings are a crucial concept in natural language processing and machine learning, offering a way to represent words, sentences, or even entire documents as vectors in a high-dimensional space. These vectors capture semantic meaning and relationships between words or phrases. Visualizing embeddings can be quite enlightening, as it helps to understand how models perceive and process language.\n","\n","### Visualizing Embeddings\n","\n","To visualize embeddings, we usually reduce their dimensionality to 2D or 3D using techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding). This allows us to plot them and observe how words with similar meanings are grouped close together.\n","\n","#### Step-by-Step Process:\n","\n","1. **Get Pre-trained Embeddings:** We'll use pre-trained word embeddings like GloVe or Word2Vec. These embeddings are trained on large corpora and capture rich language semantics.\n","\n","2. **Select Words for Visualization:** Choose a set of words that includes a mix of similar and dissimilar terms to illustrate how embeddings capture semantic relationships.\n","\n","3. **Dimensionality Reduction:** Use PCA or t-SNE to reduce the embeddings to 2 or 3 dimensions.\n","\n","4. **Plotting:** Plot the reduced embeddings and annotate them with corresponding words.\n","\n","### Code Example: Visualizing Word Embeddings"],"metadata":{"id":"InE36Cz4Wjqm"}},{"cell_type":"code","source":["! pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8pTnk2GXPMX","executionInfo":{"status":"ok","timestamp":1700108509769,"user_tz":480,"elapsed":8325,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"3c0ce8eb-95ad-4722-9a64-c73b55698232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n","Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from sklearn.manifold import TSNE\n","from torchtext.vocab import GloVe\n","import plotly.express as px\n","\n","# Load pre-trained GloVe embeddings\n","glove = GloVe(name='6B', dim=100)  # Using 100-dimensional vectors\n","\n","# Expanded list of words with categories\n","words = [\n","    (\"king\", \"royalty\"), (\"queen\", \"royalty\"), (\"prince\", \"royalty\"), (\"princess\", \"royalty\"), (\"duke\", \"royalty\"), (\"monarch\", \"royalty\"),\n","    (\"apple\", \"fruit\"), (\"banana\", \"fruit\"), (\"grape\", \"fruit\"), (\"orange\", \"fruit\"), (\"berry\", \"fruit\"), (\"melon\", \"fruit\"),\n","    (\"paris\", \"city\"), (\"berlin\", \"city\"), (\"london\", \"city\"), (\"madrid\", \"city\"), (\"rome\", \"city\"), (\"vienna\", \"city\"),\n","    (\"google\", \"tech\"), (\"microsoft\", \"tech\"), (\"apple\", \"tech\"), (\"ibm\", \"tech\"), (\"intel\", \"tech\"), (\"facebook\", \"tech\"),\n","    (\"happy\", \"emotion\"), (\"sad\", \"emotion\"), (\"angry\", \"emotion\"), (\"joyful\", \"emotion\"), (\"upset\", \"emotion\"), (\"glad\", \"emotion\"),\n","    (\"sweet\", \"taste?/emotion?\")\n","]\n","\n","# Extracting vectors for selected words\n","word_vectors = [glove[word] for word, _ in words]\n","word_vectors = torch.stack(word_vectors)\n","\n","# Reducing dimensions to 2D using t-SNE\n","tsne = TSNE(n_components=2, random_state=0, perplexity=len(words) // 5)\n","words_reduced = tsne.fit_transform(word_vectors)\n","\n","# Convert the t-SNE output to a DataFrame\n","df = pd.DataFrame(words_reduced, columns=['x', 'y'])\n","df['word'] = [word for word, _ in words]\n","df['category'] = [category for _, category in words]\n","\n","# Create a 2D scatter plot using Plotly Express with colors for categories\n","fig = px.scatter(df, x='x', y='y', text='word', color='category',\n","                 title=\"2D Visualization of GloVe Word Embeddings with Categorical Colors\")\n","fig.update_traces(textposition='top center')\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"gZ0khkchZf7D","executionInfo":{"status":"ok","timestamp":1700112940323,"user_tz":480,"elapsed":1320,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"f03c25ac-561d-4083-fb9f-5edaf6d54939"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"572dfdb5-7a8f-4f82-b975-e487ea08575d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"572dfdb5-7a8f-4f82-b975-e487ea08575d\")) {                    Plotly.newPlot(                        \"572dfdb5-7a8f-4f82-b975-e487ea08575d\",                        [{\"hovertemplate\":\"category=royalty\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"royalty\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"royalty\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"king\",\"queen\",\"prince\",\"princess\",\"duke\",\"monarch\"],\"x\":[14.439229011535645,13.88394546508789,18.02484893798828,17.004297256469727,19.747467041015625,10.541876792907715],\"xaxis\":\"x\",\"y\":[-21.01647186279297,-25.745773315429688,-22.167709350585938,-27.21297836303711,-18.438196182250977,-22.50902557373047],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"},{\"hovertemplate\":\"category=fruit\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"fruit\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"fruit\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"apple\",\"banana\",\"grape\",\"orange\",\"berry\",\"melon\"],\"x\":[-29.111736297607422,-21.18595314025879,-17.104936599731445,-24.7938289642334,-20.694578170776367,-17.53221321105957],\"xaxis\":\"x\",\"y\":[15.443735122680664,8.996726989746094,10.604187965393066,4.759525299072266,4.166901588439941,6.409558296203613],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"},{\"hovertemplate\":\"category=city\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"city\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"city\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"paris\",\"berlin\",\"london\",\"madrid\",\"rome\",\"vienna\"],\"x\":[40.80839157104492,37.474700927734375,43.01253128051758,39.79111099243164,36.470890045166016,34.92056655883789],\"xaxis\":\"x\",\"y\":[-3.866908073425293,-8.089495658874512,-6.927502155303955,1.035400629043579,-2.595808506011963,-6.418307304382324],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"},{\"hovertemplate\":\"category=tech\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"tech\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"tech\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"google\",\"microsoft\",\"apple\",\"ibm\",\"intel\",\"facebook\"],\"x\":[-32.508087158203125,-34.036277770996094,-31.004222869873047,-38.463138580322266,-37.070594787597656,-30.388362884521484],\"xaxis\":\"x\",\"y\":[23.57655143737793,19.756357192993164,13.305617332458496,19.313364028930664,16.026714324951172,26.32210350036621],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"},{\"hovertemplate\":\"category=emotion\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003eword=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"emotion\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"emotion\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"happy\",\"sad\",\"angry\",\"joyful\",\"upset\",\"glad\"],\"x\":[5.5171613693237305,3.7418878078460693,10.074962615966797,0.6392684578895569,10.404987335205078,5.2670512199401855],\"xaxis\":\"x\",\"y\":[0.9644361138343811,4.7222747802734375,4.255085468292236,1.910427451133728,-0.012034417130053043,-1.9786425828933716],\"yaxis\":\"y\",\"type\":\"scatter\",\"textposition\":\"top center\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"category\"},\"tracegroupgap\":0},\"title\":{\"text\":\"2D Visualization of GloVe Word Embeddings with Categorical Colors\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('572dfdb5-7a8f-4f82-b975-e487ea08575d');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Explanation\n","- **Loading GloVe Embeddings:** torchtext's GloVe class automatically handles downloading the GloVe embeddings. Here, we use the 100-dimensional vectors from the '6B' GloVe dataset.\n","- **Selecting Words:** We choose a set of diverse words to visualize how their embeddings relate to each other in a 2D space.\n","- **Dimensionality Reduction with t-SNE:** t-SNE is used to reduce the dimensions of the embeddings to 2D for visualization purposes.\n","- **Plotting:** We plot these reduced dimensions and annotate them with the corresponding words."],"metadata":{"id":"JoZir5SMXdcm"}},{"cell_type":"markdown","source":["# Self- Attention and the Transformer\n","This brings us to Self-Attention and the Transformer.\n","\n","Transitioning to self-attention involves understanding that, unlike LSTMs, transformers process the entire input sequence simultaneously, allowing them to capture dependencies between words regardless of their position in the sentence. This is done through self-attention mechanisms which compute a weighted sum of all words in the sentence, with the weights signifying the relevance of other words when encoding a particular word.\n","\n","### Introduction to Self-Attention\n","\n","Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence. It has been effectively used in tasks where the entire context of the sequence is important.\n","\n","The self-attention mechanism allows the model to weigh the influence of different parts of the input data differently. This is particularly useful for handling sequences with varying lengths and complex relationships between elements.\n","\n","### Building a Basic Transformer Model\n","\n","Here we'll outline a basic version of the Transformer model introduced in paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), focusing on the key components that make up the transformer architecture.\n","\n","#### Transformer Model Components:\n","\n","1. **Encoder and Decoder:** The transformer model consists of an encoder to process the input and a decoder to produce the output.\n","2. **Self-Attention Layer:** In both the encoder and decoder, self-attention layers compute attention scores for each element in the sequence.\n","3. **Feed-Forward Neural Networks:** Each self-attention layer is followed by a feed-forward neural network.\n","4. **Positional Encoding:** Since the transformer doesn't have any recurrent or convolutional structures, positional encodings are added to give the model information about the position of each token in the sequence.\n"],"metadata":{"id":"uRI4LJtjVZ69"}},{"cell_type":"markdown","source":["### Transformer Model Code Example\n","\n","Let's build a simple version of the transformer model using PyTorch:\n"],"metadata":{"id":"BgKKSyzZVxrY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embed size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        # Split the embedding into self.heads different pieces\n","        N = query.shape[0]\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)\n","        keys = self.keys(keys)\n","        queries = self.queries(queries)\n","\n","        # Attention calculation\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","\n","        out = self.fc_out(out)\n","        return out\n"],"metadata":{"id":"i9btNR5OQUvj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","This `SelfAttention` module can then be integrated into the encoder and decoder of a `transformer` architecture. The full transformer architecture is quite complex and would require additional components such as multi-head attention (which the above class handles), layer normalization, and more.\n","\n","### Explanation of the Transformer Code\n","\n","- **Self Attention Class:** This class is a simplified version of the multi-head self-attention mechanism. It computes the attention scores and applies them to the values.\n","- **Forward Method:** The method takes queries, keys, and values, splits them into multiple heads, and applies the self-attention mechanism. It outputs a weighted sum of values, combined from each head's results.\n","- **Masking:** If a mask is provided (used for padding or future blinding in the decoder), it is applied to the attention scores to prevent the model from attending to certain positions.\n","\n","This basic self-attention module is a core building block of the transformer model, allowing it to consider the entire sequence at once and learn the dependencies between all tokens, regardless of their distance in the sequence. It demonstrates how transformers can overcome the limitations of RNNs and"],"metadata":{"id":"KbIKyNf-V0zo"}},{"cell_type":"markdown","source":["### From Self Attention to Global Attention\n","\n","[Attention Family Tree](https://ai.v-gar.de/ml/transformer/timeline/)"],"metadata":{"id":"oIU7Fe36ciCR"}},{"cell_type":"markdown","source":["The journey from the self-attention mechanism in models like BERT to the development of large-scale generative AI models like GPT-3 and GPT-4 involved significant advancements in neural network architectures, computational resources, and training methodologies.\n","\n","### 1. **Advancements Post 'Attention Is All You Need' Paper**\n","\n","After the introduction of the Transformer architecture in the \"Attention Is All You Need\" paper, there was a surge in research focusing on leveraging self-attention for various tasks.\n","\n","- **BERT and Its Impact:** BERT (Bidirectional Encoder Representations from Transformers), introduced by Google, was a landmark in understanding how context in language can be used for tasks like question answering and language inference. Unlike previous models that processed text in one direction (either left-to-right or right-to-left), BERT was designed to understand the context of a word in relation to all other words in a sentence (bidirectionality).\n","\n","### 2. **From Understanding to Generation: GPT Series**\n","\n","While BERT was focused on understanding language (natural language understanding, NLU), the next big leap was towards language generation (natural language generation, NLG).\n","\n","- **GPT Series:** OpenAI’s GPT (Generative Pretrained Transformer) models shifted the focus towards generative tasks. GPT models are trained to predict the next word in a sentence, given the words that come before it. This predictive modeling can be extended to generate coherent and contextually relevant text over longer passages.\n","  \n","  - **GPT-1 and GPT-2:** These models demonstrated that a large-scale Transformer trained on a diverse range of internet text could generate coherent and surprisingly relevant text snippets based on given prompts.\n","  \n","  - **GPT-3 and Beyond:** With GPT-3, the scale was dramatically increased - both in terms of the size of the model (number of parameters) and the diversity and volume of training data. GPT-3 showed that scaling up the size of the model and the training data led to a significant increase in the model's ability to generate coherent and contextually appropriate text, as well as perform a variety of language tasks without task-specific training data (few-shot or zero-shot learning).\n","\n","### 3. **Computational Power and Data**\n","\n","The leap to models like GPT-3 and GPT-4 also required massive computational resources:\n","\n","- **Hardware Advancements:** The development of more powerful GPUs and TPUs allowed for training larger models with billions of parameters more efficiently.\n","- **Larger and More Diverse Datasets:** The availability of large-scale and diverse datasets facilitated training models that could understand and generate more nuanced and contextually varied text.\n","\n","### 4. **Broader Applications and Fine-Tuning**\n","\n","With powerful generative models, the range of applications expanded significantly:\n","\n","- **Versatility:** Models like GPT-3 can be fine-tuned for specific tasks (like translation, summarization, content creation) or used directly with prompts for various applications, demonstrating a broad understanding of language and knowledge.\n","- **Emergence of AI as a Service:** The introduction of these models as cloud services made powerful AI accessible to a broader audience, fueling a wave of innovation in AI applications.\n","\n","### 5. **Continued Research and Ethical Considerations**\n","\n","- **Ongoing Research:** The field continues to evolve, with research focusing on making models more efficient, less biased, and more interpretable.\n","- **Ethical and Societal Impact:** The rise of powerful generative models has also brought attention to ethical considerations, including potential misuse, bias in AI, and the impact on various sectors like journalism, law, and creative industries.\n","\n","The transition from BERT's breakthrough in understanding context to the generative prowess of GPT-3 and GPT-4 marks a significant era in AI's evolution, highlighting how advancements in model architecture, training techniques, and computational capabilities come together to push the boundaries of what's possible in natural language processing and AI at large."],"metadata":{"id":"1yFS85C2cuOp"}},{"cell_type":"code","source":[],"metadata":{"id":"xwTAkZiWV0Qu"},"execution_count":null,"outputs":[]}]}