{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EIDvTq3MulGe5c42ANE-F8stQvXTZycG","authorship_tag":"ABX9TyO0ldO7w12558o+KWY1bKNx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Student and Problem Set Info\n","---\n","\n","\n","## Title: MGSC 310: Problem Set 4\n","\n","Author:\n","\n","Ben Labaschin, King of the Notebooks, Destroyer of Worlds.\n"],"metadata":{"id":"iGTMJcjlG6-Y"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"LuN6N1mjJXo7"}},{"cell_type":"markdown","source":["Pre-req:\n","- Install the `ISLP package`\n","- restart your runtime\n","- load the `Caravan` dataset found [here](https://islp.readthedocs.io/en/latest/datasets/Caravan.html) (data dictionary included in documentation).\n","- assign the `Caravan` data to the variable `caravan`"],"metadata":{"id":"iA6inV9Xl8Hy"}},{"cell_type":"code","source":["! pip install ISLP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RM96HZtE9EAL","executionInfo":{"status":"ok","timestamp":1697152267732,"user_tz":420,"elapsed":5994,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"6a920c5e-fad1-4b14-a9f6-b756f74d0b4c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ISLP in /usr/local/lib/python3.10/dist-packages (0.3.21)\n","Requirement already satisfied: numpy<1.25,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.24.4)\n","Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.11.3)\n","Requirement already satisfied: pandas<=1.9,>=0.20 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.5.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from ISLP) (4.9.3)\n","Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.2.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.3.2)\n","Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from ISLP) (0.14.0)\n","Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (from ISLP) (0.27.8)\n","Requirement already satisfied: pygam in /usr/local/lib/python3.10/dist-packages (from ISLP) (0.9.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ISLP) (2.0.1+cu118)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from ISLP) (2.1.0)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (from ISLP) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.9,>=0.20->ISLP) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.9,>=0.20->ISLP) (2023.3.post1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2->ISLP) (3.2.0)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->ISLP) (0.5.3)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->ISLP) (23.2)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (3.7.1)\n","Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (1.6.2)\n","Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (0.5.0)\n","Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines->ISLP) (0.6.6)\n","Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pygam->ISLP) (4.2.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (6.0.1)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (4.5.0)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->ISLP) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->ISLP) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ISLP) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->ISLP) (17.0.2)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.5->lifelines->ISLP) (0.18.3)\n","Requirement already satisfied: astor>=0.8 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->ISLP) (0.8.1)\n","Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.3.0)\n","Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (3.8.6)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines->ISLP) (3.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13->ISLP) (1.16.0)\n","Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam->ISLP) (3.8.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ISLP) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ISLP) (1.3.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->ISLP) (2023.7.22)\n"]}]},{"cell_type":"code","source":["from ISLP import load_data\n","\n","caravan = load_data(\"Caravan\")"],"metadata":{"id":"DaEwO3VNKjaG","executionInfo":{"status":"ok","timestamp":1697152401666,"user_tz":420,"elapsed":161,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Question 1:\n","\n","1. How many purchased insurance policies were there?\n","2. If our target variable is `Purchase`, is the dataset imbalanced? If so, by precisely how much?\n","3. remap `No` values to 0 and `Yes` values to 1."],"metadata":{"id":"msP9R1t9JibX"}},{"cell_type":"code","source":["caravan['Purchase'].value_counts()\n","\n","# 1. 348 purchases\n","# 2. Yes it's imbalanced by 5474-348 or 5126 policies"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBZOntJMKvB2","executionInfo":{"status":"ok","timestamp":1697152651256,"user_tz":420,"elapsed":164,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"7b6bf0b9-8889-4d56-9df7-eb0bcd519edd"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["No     5474\n","Yes     348\n","Name: Purchase, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 3\n","mapping = {\"No\": 0, \"Yes\": 1}\n","caravan['Purchase'] = caravan['Purchase'].map(mapping)"],"metadata":{"id":"XS7docsAL8vo","executionInfo":{"status":"ok","timestamp":1697152787210,"user_tz":420,"elapsed":155,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["caravan['Purchase'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"agx2igAWMG-b","executionInfo":{"status":"ok","timestamp":1697152801315,"user_tz":420,"elapsed":159,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"6978d500-468d-43b2-d32a-1f8f40c9198f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    5474\n","1     348\n","Name: Purchase, dtype: int64"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Question 2: Balance the Data\n","\n","1. Using the `sklearn` `resample` function, **downsample** the target `Purchase` such that the 0's and 1's are balanced.\n"," - This answer will not be counted as correct unless the downsampled features are joined with the downsampled targets into a single DataFrame, as demonstrated in the class code\n"," - Assign the resulting dataframe to the variable `caravan_downsampled`"],"metadata":{"id":"oB8iZR0fLoJ9"}},{"cell_type":"code","source":["from sklearn.utils import resample\n","from pandas import concat\n","\n","# Separate the majority and minority classes\n","caravan_no = caravan[caravan['Purchase'] == 0]\n","caravan_yes = caravan[caravan['Purchase'] == 1]\n","\n","# Downsample the majority class\n","caravan_no_downsampled = resample(caravan_no,\n","                                  replace=False,\n","                                  n_samples=len(caravan_yes),\n","                                  random_state=634)\n","\n","# Combine the minority class with downsampled majority class\n","caravan_downsampled = concat([caravan_no_downsampled, caravan_yes])\n","\n","caravan_downsampled['Purchase'].value_counts()"],"metadata":{"id":"j0ia16ZxlZH-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697153181410,"user_tz":420,"elapsed":153,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"934ae0ac-2d01-4475-bfdc-63c4b01bb69b"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    348\n","1    348\n","Name: Purchase, dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Question 3: K-Fold and Ridge Regularization\n","\n","1. Run cross validation upon on the `caravan_downsampled` data, running the `sklearn` [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) estimator (model) upon each fold. Use every variable (except `Purchase`) as a feature, and `Purchase` as the target variable.\\\n","\\\n","Use the class code if you need help running k-fold and ridge regression. But if you want to make your life easier for your next problem, I would use the the `cross_validate` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). By setting its argument `cv` equal to, say 3, it's essentially k-fold cross validation with 3 folds. Then you can add `RidgeClassifier` as the estimator, and pass multiple metrics at once (`precision`, `recall`) to record the results. See [cross-validate with multiple metrics](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) for more details.\\\n","\\\n","Here is some other useful documentation in addition:\n","   - [using kfold on a model examples](https://scikit-learn.org/stable/modules/cross_validation.html)\n","   - [more examples with kfold and models](https://www.askpython.com/python/examples/k-fold-cross-validation)\n","   - [official kfold documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n","2. gather the metric results for [`precision`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [`recall`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) for each fold of the model, and append them each to a list. That is,  every result for each fold for `precision` should be appended into a list, and every result for each fold for `recall` should be appended to a list.\n"," - your result here will be two lists of your metrics\n"," - if you use `cross_validate` from above, your life will be easier, simply pass the following predefined `precision` and `recall` \"macros\" to the `cross_validate` function. Fill in the `_`'s with the proper variables.\n","\n"," ```python\n"," from sklearn.model_selection import cross_validate\n"," scoring = ['precision_macro', 'recall_macro']\n"," ... <other setup code>...\n"," scores = cross_validate(_, _, _, cv=3, scoring=scoring)\n","\n"," ```\n","\n","3. average the results of `precision`, and `recall` lists from above. Print out your scores using `print`.\n","\n"],"metadata":{"id":"BVu45VctmhXs"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_validate\n","from sklearn.linear_model import RidgeClassifier\n","\n","scoring = ['precision_macro', 'recall_macro']\n","\n","X = caravan_downsampled.drop(\"Purchase\", axis=1)\n","y = caravan_downsampled[\"Purchase\"]\n","\n","clf = RidgeClassifier()\n","scores = cross_validate(clf, X, y, cv=3, scoring=scoring)\n","scores\n"],"metadata":{"id":"eZcqXLLKmo0o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697156337995,"user_tz":420,"elapsed":173,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"bd919750-fa37-4ee8-c4cd-e2c85208f810"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'fit_time': array([0.01399279, 0.02087998, 0.01889491]),\n"," 'score_time': array([0.01860929, 0.0153892 , 0.01498532]),\n"," 'test_precision_macro': array([0.6277436 , 0.66006564, 0.63996983]),\n"," 'test_recall_macro': array([0.625     , 0.65948276, 0.63793103])}"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["print(\"Precision average: \", sum(scores['test_precision_macro'])/3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRo51bvKWcUR","executionInfo":{"status":"ok","timestamp":1697156386921,"user_tz":420,"elapsed":145,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"c7d39139-e2e5-415a-fc6b-2ea87026591c"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision average:  0.642593024289638\n"]}]},{"cell_type":"code","source":["print(\"Recall average: \",sum(scores['test_recall_macro'])/3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2wWrQuAWSnC","executionInfo":{"status":"ok","timestamp":1697156399943,"user_tz":420,"elapsed":146,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"523eeafe-2ada-49a8-9caf-05d00419dc01"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall average:  0.6408045977011495\n"]}]},{"cell_type":"markdown","source":["# Question 4: K-Fold and Ridge Regularization (Redux)\n","\n","1. Run the EXACT same code as above *except* run it on the `caravan` variable that hasn't been downsampled. i.e.\n"," - Run cross validation upon on the **`caravan`** data, running the `sklearn` [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) estimator (model) upon each fold. Use every variable (except `Purchase`) as a feature, and `Purchase` as the target variable.\\\n"," \\\n"," Use the class code if you need help running k-fold and ridge regression. But if you want to make your life easier for your next problem, I would use the the `cross_validate` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). By setting its argument `cv` equal to, say 3, it's essentially k-fold cross validation with 3 folds. Then you can add `RidgeClassifier` as the estimator, and pass multiple metrics at once (`precision`, `recall`) to record the results. See [cross-validate with multiple metrics](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) for more details.\\\n"," \\\n"," Here is some other useful documentation in addition:\n","    - [using kfold on a model examples](https://scikit-learn.org/stable/modules/cross_validation.html)\n","    - [more examples with kfold and models](https://www.askpython.com/python/examples/k-fold-cross-validation)\n","    - [official kfold documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n"," - gather the metric results for [`precision`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [`recall`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) for each fold of the model, and append them each to a list. That is,  every result for each fold for `precision` should be appended into a list, and every result for each fold for `recall` should be appended to a list.\n","  - your result here will be two lists of your metrics\n","  - if you use `cross_validate` from above, your life will be easier, simply pass the following predefined `precision` and `recall` \"macros\" to the `cross_validate` function. Fill in the `_`'s with the proper variables.\n","\n","    ```python\n","    from sklearn.model_selection import cross_validate\n","    scoring = ['precision_macro', 'recall_macro']\n","    ... <other setup code>...\n","    scores = cross_validate(_, _, _, cv=3, scoring=scoring)\n","    ```\n","\n"," - average the results of `precision`, and `recall` lists from above. Print out your scores using `print`.\n","\n"],"metadata":{"id":"8Qso2L89aKH-"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_validate\n","from sklearn.linear_model import RidgeClassifier\n","\n","scoring = ['precision_macro', 'recall_macro']\n","\n","X = caravan.drop(\"Purchase\", axis=1)\n","y = caravan[\"Purchase\"]\n","\n","clf = RidgeClassifier()\n","scores = cross_validate(clf, X, y, cv=3, scoring=scoring)\n","scores\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4z20C4jaAXP","executionInfo":{"status":"ok","timestamp":1697156668356,"user_tz":420,"elapsed":338,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"a5c893a0-69ed-46cd-9b5c-94c7cd2af6cf"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'fit_time': array([0.01863623, 0.02966475, 0.0284276 ]),\n"," 'score_time': array([0.01331091, 0.01157928, 0.01239777]),\n"," 'test_precision_macro': array([0.47005679, 0.97036082, 0.47008767]),\n"," 'test_recall_macro': array([0.49890411, 0.50431034, 0.49972588])}"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["print(\"Precision average: \", sum(scores['test_precision_macro'])/3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xu7pUGWna8MM","executionInfo":{"status":"ok","timestamp":1697156686224,"user_tz":420,"elapsed":150,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"13efe7d3-5061-4a40-a1d3-c936f44bba4e"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision average:  0.6368350958832655\n"]}]},{"cell_type":"code","source":["print(\"Recall average: \",sum(scores['test_recall_macro'])/3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7s_Kej6a-YB","executionInfo":{"status":"ok","timestamp":1697156688965,"user_tz":420,"elapsed":6,"user":{"displayName":"Ben Labaschin","userId":"07831406811640108386"}},"outputId":"c4ebc90c-d1b5-4423-c07b-cc117c6deba7"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall average:  0.5009801105365366\n"]}]},{"cell_type":"markdown","source":["# Question 5: Examining Our Results\n","\n","1. Look at the average results for the downsampled data and the unaltered data. Which performed better overall?\n","2. Why do you think the it was that this dataset performed better?\n","3. Let's say we cared about predicting who would purchase insurance and the false positives weren't that costly to us (we're okay with being wrong), which dataset (the downsampled data vs the untouched data) and which specific metric (precision or recall)  would you use and why? i.e. downsampled (precision/recall) or untouched (precision/recall)"],"metadata":{"id":"3s82A255bBbe"}},{"cell_type":"markdown","source":["1. The downsampled data performed better. Both precision and recall performed better on average, and even the variance within each fold was tighter.\n","2. By reducing the \"noise\" of the underlying dataset, randomly, and balancing the target data, the model was able to find more signal in the data—it wasn't trained on so much data that was irrelevant that it started finding patterns we didn't want. Further, when we downsample our data, we are reducing the chance that the model fits to the \"no purchase\" instance, which we care less about than the \"purchase\" instance.\n","3. Downsample data, recall. Since false positives aren't that important to us, and precision uses false positives in its denominator, precision is weighing information we care less about. Recall cares about True Positives and False Negatives. Its focus is more about how positive instances we actually labeled correctly, and that's what we care about."],"metadata":{"id":"z1qug7DWb8Gq"}}]}