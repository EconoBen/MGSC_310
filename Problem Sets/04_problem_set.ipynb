{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EIDvTq3MulGe5c42ANE-F8stQvXTZycG","authorship_tag":"ABX9TyNi74vjIrrwuCcRf8TyNE7S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Student and Problem Set Info\n","---\n","\n","\n","## Title: MGSC 310: Problem Set 4\n","\n","Author:\n","\n","<Your name here>"],"metadata":{"id":"iGTMJcjlG6-Y"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"LuN6N1mjJXo7"}},{"cell_type":"markdown","source":["Pre-req:\n","- Install the `ISLP package`\n","- restart your runtime\n","- load the `Caravan` dataset found [here](https://islp.readthedocs.io/en/latest/datasets/Caravan.html) (data dictionary included in documentation).\n","- assign the `Caravan` data to the variable `caravan`"],"metadata":{"id":"iA6inV9Xl8Hy"}},{"cell_type":"markdown","source":["## Question 1:\n","\n","1. How many purchased insurance policies were there?\n","2. If our target variable is `Purchase`, is the dataset imbalanced? If so, by precisely how much?\n","3. remap `No` values to 0 and `Yes` values to 1."],"metadata":{"id":"msP9R1t9JibX"}},{"cell_type":"markdown","source":["# Question 2: Balance the Data\n","\n","1. Using the `sklearn` `resample` function, **downsample** the target `Purchase` such that the 0's and 1's are balanced.\n"," - This answer will not be counted as correct unless the downsampled features are joined with the downsampled targets into a single DataFrame, as demonstrated in the class code\n"," - Assign the resulting dataframe to the variable `caravan_downsampled`"],"metadata":{"id":"oB8iZR0fLoJ9"}},{"cell_type":"markdown","source":["# Question 3: K-Fold and Ridge Regularization\n","\n","1. Run cross validation upon on the `caravan_downsampled` data, running the `sklearn` [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) estimator (model) upon each fold. Use every variable (except `Purchase`) as a feature, and `Purchase` as the target variable.\\\n","\\\n","Use the class code if you need help running k-fold and ridge regression. But if you want to make your life easier for your next problem, I would use the the `cross_validate` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). By setting its argument `cv` equal to, say 3, it's essentially k-fold cross validation with 3 folds. Then you can add `RidgeClassifier` as the estimator, and pass multiple metrics at once (`precision`, `recall`) to record the results. See [cross-validate with multiple metrics](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) for more details.\\\n","\\\n","Here is some other useful documentation in addition:\n","   - [using kfold on a model examples](https://scikit-learn.org/stable/modules/cross_validation.html)\n","   - [more examples with kfold and models](https://www.askpython.com/python/examples/k-fold-cross-validation)\n","   - [official kfold documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n","2. gather the metric results for [`precision`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [`recall`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) for each fold of the model, and append them each to a list. That is,  every result for each fold for `precision` should be appended into a list, and every result for each fold for `recall` should be appended to a list.\n"," - your result here will be two lists of your metrics\n"," - if you use `cross_validate` from above, your life will be easier, simply pass the following predefined `precision` and `recall` \"macros\" to the `cross_validate` function. Fill in the `_`'s with the proper variables.\n","\n"," ```python\n"," from sklearn.model_selection import cross_validate\n"," scoring = ['precision_macro', 'recall_macro']\n"," ... <other setup code>...\n"," scores = cross_validate(_, _, _, cv=3, scoring=scoring)\n","\n"," ```\n","\n","3. average the results of `precision`, and `recall` lists from above. Print out your scores using `print`.\n","\n"],"metadata":{"id":"BVu45VctmhXs"}},{"cell_type":"markdown","source":["# Question 4: K-Fold and Ridge Regularization (Redux)\n","\n","1. Run the EXACT same code as above *except* run it on the `caravan` variable that hasn't been downsampled. i.e.\n"," - Run cross validation upon on the **`caravan`** data, running the `sklearn` [`RidgeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) estimator (model) upon each fold. Use every variable (except `Purchase`) as a feature, and `Purchase` as the target variable.\\\n"," \\\n"," Use the class code if you need help running k-fold and ridge regression. But if you want to make your life easier for your next problem, I would use the the `cross_validate` function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate). By setting its argument `cv` equal to, say 3, it's essentially k-fold cross validation with 3 folds. Then you can add `RidgeClassifier` as the estimator, and pass multiple metrics at once (`precision`, `recall`) to record the results. See [cross-validate with multiple metrics](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation) for more details.\\\n"," \\\n"," Here is some other useful documentation in addition:\n","    - [using kfold on a model examples](https://scikit-learn.org/stable/modules/cross_validation.html)\n","    - [more examples with kfold and models](https://www.askpython.com/python/examples/k-fold-cross-validation)\n","    - [official kfold documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n"," - gather the metric results for [`precision`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [`recall`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) for each fold of the model, and append them each to a list. That is,  every result for each fold for `precision` should be appended into a list, and every result for each fold for `recall` should be appended to a list.\n","  - your result here will be two lists of your metrics\n","  - if you use `cross_validate` from above, your life will be easier, simply pass the following predefined `precision` and `recall` \"macros\" to the `cross_validate` function. Fill in the `_`'s with the proper variables.\n","\n","    ```python\n","    from sklearn.model_selection import cross_validate\n","    scoring = ['precision_macro', 'recall_macro']\n","    ... <other setup code>...\n","    scores = cross_validate(_, _, _, cv=3, scoring=scoring)\n","    ```\n","\n"," - average the results of `precision`, and `recall` lists from above. Print out your scores using `print`.\n","\n"],"metadata":{"id":"8Qso2L89aKH-"}},{"cell_type":"markdown","source":["# Question 5: Examining Our Results\n","\n","1. Look at the average results for the downsampled data and the unaltered data. Which performed better overall?\n","2. Why do you think it was that this dataset performed better?\n","3. Let's say we cared about predicting who would purchase insurance and the false positives weren't that costly to us (we're okay with being wrong), which dataset (the downsampled data vs the untouched data) and which specific metric (precision or recall)  would you use and why? i.e. downsampled (precision/recall) or untouched (precision/recall)"],"metadata":{"id":"3s82A255bBbe"}}]}